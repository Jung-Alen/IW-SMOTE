# author：Aimin Zhang，Hualong Yu
# Time:2022/4/2
# Institution：Jiangsu University of Science and Technology


#!/usr/bin/env python
# coding: utf-8

# In[1]:


import numpy as np
import pandas as pd
import smote_variants as sv
import scipy.io as io
import random
import CSELM as ce
from sklearn.tree import DecisionTreeClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import accuracy_score
from sklearn.svm import SVC
import sklearn.metrics
from sklearn.metrics import accuracy_score
from sklearn.metrics import  precision_score
from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import cohen_kappa_score
from sklearn.metrics import f1_score
from sklearn.metrics import roc_curve
from sklearn.metrics import auc
import matplotlib.pyplot as plt
import math
import os
from sklearn.preprocessing import StandardScaler
from sklearn.preprocessing import MinMaxScaler
from matplotlib import gridspec
from   sklearn.model_selection  import train_test_split
from sklearn.datasets import load_iris
from sklearn.neighbors import KNeighborsClassifier
from sklearn import metrics
from sklearn.model_selection import cross_val_score
from time import *
from sklearn.neighbors import KNeighborsClassifier

#the classfication and regression tree
def CART(X, y, XX):
    model = DecisionTreeClassifier()
    model.fit(X, y)
    predicted = model.predict(XX)
    return predicted


class Compared_smote():
    def __init__(self, balance=1):
        self.balance = balance;#Sampling rate


    def IW_SMOTE(self, lamda=100,  thres=0.5, k_neighbor=5, divide_times=2, gen_times=1， data, target):
        """
            :param lamda: lamda*imbalance ratio = the number of cart
            :param thres: The noise threshold
            :param k_neighbor: k nearest neighbor
            :param divide_times: The ratio of under sampling minority samples
            :param gen_times: The ratio of generated minority class to majority class samples
            :param data: train data
            :param target: the labels of train data
        """
        self.lamda = lamda;
        self.thres = thres;
        self.k_neighbor = k_neighbor;
        self.divide_times = divide_times;
        self.gen_times = gen_times;
        self.data = data
        self.target = target
        test = pd.DataFrame(self.data)
        test[len(test.columns)] = self.target
        x_x = test #x_x:Temporary variable
        m, n = len(x_x), len(x_x.columns)
        z = x_x[x_x[n - 1] == 1]  # acquire the minority set
        p = x_x[x_x[n - 1] == -1]  # acquire the majority set
        m1, n1 = len(z), len(z.columns)
        m2, n2 = len(p), len(p.columns)
        IR = m2/m1 #imbalance ratio
        predict_min_labelset = pd.DataFrame(co lumns=range(int(IR * self.lamda)))
        predict_maj_labelset = pd.DataFrame(columns=range(int(IR * self.lamda)))
        #train under-bagging CART
        for i_1 in range(int(IR * self.lamda)):
            train_subset = z.sample(int(m1 / self.divide_times))
            train_subset = train_subset.append(p.sample(int(m1 / self.divide_times), replace=True))
            predict_maj_labelset[i_1] = CART(np.array(train_subset.iloc[:, 0:n1 - 1]), np.array(train_subset[n1 - 1]),
                                             np.array(p.iloc[:, 0:n2 - 1]))
            predict_min_labelset[i_1] = CART(np.array(train_subset.iloc[:, 0:n1 - 1]), np.array(train_subset[n1 - 1]),
                                             np.array(z.iloc[:, 0:n1 - 1]))
        # filterring noise
        err_rate_min = []  #  record the error rate of the reserved minority instance
        reserve_min = []  #  record the reserved minority instances
        num_reserve_min = 0 # the number of minority samples after denoising
        z1 = np.array(z)
        predict_min_labelset = np.array(predict_min_labelset)
        for i_2 in range(m1):
            num_right = 0  # record the number of instance which is predicted accurately
            for j in range(int(IR * self.lamda)):
                if predict_min_labelset[i_2][j] == z1[i_2][n1 - 1]:
                    num_right = num_right + 1
            if ((int(IR * self.lamda) - num_right) / int(IR * self.lamda) < self.thres):
                num_reserve_min += 1
                reserve_min.append(z1[i_2])
                if (int(IR * self.lamda) - num_right) / int(IR * self.lamda) < 1 / int(IR * self.lamda):
                    err_rate_min.append(1 / int(IR * self.lamda))
                else:
                    err_rate_min.append((int(IR * self.lamda) - num_right) / int(IR * self.lamda))
        reserve_min = pd.DataFrame(reserve_min)

        err_rate_min = pd.DataFrame(err_rate_min)
        err_rate_maj = []  #  record the error rate of the reserved minority instance
        reserve_maj = []  #  record the reserved minority instances
        num_reserve_maj = 0 # the number of majority samples after denoising
        p1 = np.array(p)
        predict_maj_labelset = np.array(predict_maj_labelset)
        for i_3 in range(m2):
            num_right = 0  # record the number of instance which is predicted accurately
            for j in range(int(IR * self.lamda)):
                if predict_maj_labelset[i_3][j] == p1[i_3][n2 - 1]:
                    num_right = num_right + 1
            if ((int(IR * self.lamda) - num_right) / int(IR * self.lamda) < self.thres):
                num_reserve_maj += 1
                reserve_maj.append(p1[i_3])
                if (int(IR * self.lamda) - num_right) / int(IR * self.lamda) < 1 / int(IR * self.lamda):
                    err_rate_maj.append(1 / int(IR * self.lamda))
                else:
                    err_rate_maj.append((int(IR * self.lamda) - num_right) / int(IR * self.lamda))
        reserve_maj = pd.DataFrame(reserve_maj)

        # generate the synthetic minority instances
        weight = err_rate_min[0] / sum(err_rate_min[0])
        num_need_generate = self.gen_times * num_reserve_maj - num_reserve_min
        if num_need_generate == num_reserve_maj:
            return np.array(reserve_maj.iloc[:, 0:len(reserve_maj.columns) - 1]), np.array(
                reserve_maj[len(reserve_maj.columns) - 1])
        else:
            num_generate = 0
            new_set = pd.DataFrame(columns=range(n1))
            for i_4 in range(num_reserve_min):
                reserve_min_1 = reserve_min
                nums = pd.DataFrame(weight * (self.gen_times * num_reserve_maj - num_reserve_min)).iloc[i_4, 0]
                reserve_min_1 = np.array(reserve_min_1)
                dis = [0] * num_reserve_min
                for m in range(num_reserve_min):
                    dis[m] = np.linalg.norm(reserve_min_1[i_4] - reserve_min_1[m])
                b = sorted(enumerate(dis), key=lambda xxx: xxx[1])
                b = b[1:self.k_neighbor + 1]
                # reserve_min = pd.DataFrame(reserve_min)
                # print(reserve_min)
                for j in range(int(nums)):
                    num_generate = num_generate + 1
                    s_b = random.choice(b)
                    select_ins = reserve_min.iloc[s_b[0], :]
                    new_ins = (reserve_min.iloc[i_4, :] - pd.DataFrame(select_ins).T) * random.random() + pd.DataFrame(
                        select_ins).T  # generate funtion
                    new_set = new_set.append(pd.DataFrame(new_ins))  # add the new instance into a temporary set
            new_z = reserve_min.append(new_set)
            new_original_data = reserve_maj.append(new_z)
            new_original_data.index = range(len(new_original_data))
            # Returns an oversampled dataset
            return np.array( new_original_data.iloc[:, 0:len(new_original_data.columns) - 1]), np.array(new_original_data[len(new_original_data.columns) - 1])

    
    
    def smote(self, data, target):
        self.data = data #train data
        self.target = target #train label
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.SMOTE(proportion = self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]), np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def b1_smote(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.Borderline_SMOTE1(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def b2_smote(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.Borderline_SMOTE2(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def sl_smote(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.Safe_Level_SMOTE(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def sn_smote(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.SN_SMOTE(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def mwmote(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.MWMOTE(proportion=1.0, k1=5, k2=3, k3=int(len(df[df[len(df.columns)-1]==1])/2), M=3, cf_th=5.0, cmax=2.0, proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def adasyn(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.ADASYN(n_neighbors=5, d_th=0.75, beta=1.0, proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def smote_enn(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.SMOTE_ENN(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def smote_tl(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.SMOTE_TomekLinks(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp

    def smote_IPF(self, data, target):
        self.data = data
        self.target = target
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        oversampler = sv.SMOTE_IPF(n_neighbors=5, n_folds=9, k=3, p=0.01, voting='consensus', proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        return X_samp, y_samp
        
        
         def smote_cselm(self, data, target, test):
        self.data = data
        self.target = target
        self.test = test
        df = pd.DataFrame(self.data)
        df[len(df.columns)] = self.target
        x_x = df
        m, n = len(x_x), len(x_x.columns)
        z = x_x[x_x[n - 1] == 1]  # acquire the minority set
        p = x_x[x_x[n - 1] == -1]  # acquire the majority set
        oversampler = sv.SMOTE(proportion=self.balance)
        X_samp, y_samp = oversampler.sample(np.array(df.iloc[:, 0:len(df.columns) - 1]),
                                            np.array(df[len(df.columns) - 1]))
        syn = pd.DataFrame(X_samp)
        syn[len(syn.columns)] = y_samp
        syn = pd.cancat([z,p,syn])
        syn = syn.drop_duplicates(keep=False)
        cse = ce.ELM(z.iloc[:,:n-1], z.iloc[:,:n-1], syn.iloc[:,:n-1], 100)
        cse.classifisor_train(z[n-1].append(p[n-1]).append(syn[n-1]))
        return  cse.classifisor_test(test)

    


